{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS376 - project demo2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalebmes/ML-and-DL-projects/blob/main/Facial%20expression%20recognition%20project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTsgbVssK_JO",
        "outputId": "bc45e69c-ccf9-4b2b-8f70-1cd3c4aabc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "#install necessary modules\n",
        "!pip install kaggle\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount your drive first - you can do it once\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hbOvpcyD-7H",
        "outputId": "c4e36efd-8800-4343-8ffb-f4b0e9c0b958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#before executing the code cell below, first download the 'kaggle.json' file from your kaggle's account and upload it to your drive:\n",
        "#steps\n",
        "#go to https://www.kaggle.com/<your-kaggle-username>/account -> example.... kaggle.com/kalebasfaw/account\n",
        "#then ctrl + F => 'create new API token'\n",
        "#then after downloading that file, upload it to your google drive"
      ],
      "metadata": {
        "id": "9NzGa9dO6SuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "HJIr3YaGLOZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_tkbgrdLTK4",
        "outputId": "f831df4b-2cb4-4beb-852a-4a9be4cca58d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 93% 266M/285M [00:01<00:00, 216MB/s]\n",
            "100% 285M/285M [00:01<00:00, 203MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's get the csv files from the zip file\n",
        "data_path = \"/content/challenges-in-representation-learning-facial-expression-recognition-challenge.zip\"\n",
        "from zipfile import ZipFile\n",
        "with ZipFile(data_path) as myzip:\n",
        "  train_data = myzip.open('train.csv')\n",
        "  test_data = myzip.open('test.csv') #this is used for testing the data, it doesn't contain the labels\n",
        "  "
      ],
      "metadata": {
        "id": "faPhOfInLnPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "1O_7o8yiL7fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading our data into pandas data frame\n",
        "train_df = pd.read_csv(train_data)\n",
        "test_df = pd.read_csv(test_data) #not used for this task, only for testing, and ranking"
      ],
      "metadata": {
        "id": "5GCQVfG8L_kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle indices into training set and test sets and separate the validation data from the training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_df['pixels'], train_df['emotion'], test_size=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "StJnIEzrMG-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert each of the training and the validation datasets into dataframes\n",
        "new_train = pd.DataFrame(data=X_train)\n",
        "new_train['emotion'] = y_train\n",
        "new_val = pd.DataFrame(data=X_val)\n",
        "new_val['emotion'] = y_val"
      ],
      "metadata": {
        "id": "3EN8jIJvMOGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now make directories to store each of the images of each categories from the pixels\n",
        "os.mkdir('/content/fer_images')\n",
        "os.mkdir('/content/fer_images/train')\n",
        "os.mkdir('/content/fer_images/validation')"
      ],
      "metadata": {
        "id": "n89ADChvMb-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WARNING - EXECUTE THIS CELL ONLY ONCE, create a directory of each of the emotions\n",
        "em_dict = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'sad', 5: 'surprise', 6: 'neutral'}\n",
        "fer_path = os.path.join('/content', 'fer_images')\n",
        "os.chdir(fer_path)\n",
        "train_path = os.path.join(fer_path, 'train')\n",
        "validation_path = os.path.join(fer_path, 'validation')\n",
        "for key in em_dict.keys():\n",
        "  os.chdir(train_path)\n",
        "  os.mkdir(em_dict[key])\n",
        "  os.chdir(validation_path)\n",
        "  os.mkdir(em_dict[key])"
      ],
      "metadata": {
        "id": "IVtOdAIhNDkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the images into each of the folders of their respective emotions\n",
        "##saving each images on the training set\n",
        "counter_each = {'angry': 0, 'sad': 0, 'happy': 0, 'neutral': 0, 'disgust': 0, 'fear': 0, 'surprise': 0}\n",
        "for (ind, row) in new_train.iterrows():\n",
        "  emotion = em_dict[row['emotion']]\n",
        "  emotion_path = os.path.join(train_path, emotion)\n",
        "  img = np.fromstring(row['pixels'], dtype='uint8', sep=' ')\n",
        "  img = img.reshape((48, 48))\n",
        "  image = Image.fromarray(img)\n",
        "  counter_each[emotion] += 1\n",
        "  filename = f'{emotion}{counter_each[emotion]}.jpg'\n",
        "  image_path = os.path.join(emotion_path, filename)\n",
        "  image.save(image_path)\n",
        "  # print('saved: ', filename, 'in the training set on the path: ', image_path)"
      ],
      "metadata": {
        "id": "DwPgct7lMpzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##saving each images on the validation set\n",
        "counter_each = {'angry': 0, 'sad': 0, 'happy': 0, 'neutral': 0, 'disgust': 0, 'fear': 0, 'surprise': 0}\n",
        "for (ind, row) in new_val.iterrows():\n",
        "  emotion = em_dict[row['emotion']]\n",
        "  emotion_path = os.path.join(validation_path, emotion)\n",
        "  img = np.fromstring(row['pixels'], dtype='uint8', sep=' ')\n",
        "  img = img.reshape((48, 48))\n",
        "  image = Image.fromarray(img)\n",
        "  counter_each[emotion] += 1\n",
        "  filename = f'{emotion}{counter_each[emotion]}.jpg'\n",
        "  image_path = os.path.join(emotion_path, filename)  \n",
        "  image.save(image_path)\n",
        "  # print('saved: ', filename, 'in the validation set on the path: ', image_path)"
      ],
      "metadata": {
        "id": "heRaWeHeNoFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "lr = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "\n",
        "#device\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  print('GPU available')\n",
        "else:\n",
        "  print('training is done on CPU')"
      ],
      "metadata": {
        "id": "1AtbkaA7Ozsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b916acf0-d5dc-4164-cc44-62418cfad8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's load the dataset into the 'dataloader' package of pytorch\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "mDauIl3KPaE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first step is creating an augmentation cell using transforms of pytorch\n",
        "train_augs = T.Compose([T.RandomHorizontalFlip(p=0.5), \n",
        "                        T.RandomRotation(degrees=(-20, +20)), \n",
        "                        T.ToTensor()])\n",
        "\n",
        "valid_augs = T.Compose([T.ToTensor()])"
      ],
      "metadata": {
        "id": "3Mn5MAN8SsUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = ImageFolder(train_path, transform=train_augs)\n",
        "validset = ImageFolder(validation_path, transform=valid_augs)\n",
        "print(f\"Total no. of examples in trainset : {len(trainset)}\")\n",
        "print(f\"Total no. of examples in validset : {len(validset)}\")"
      ],
      "metadata": {
        "id": "-iRhgKwIS3jX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d98b5c6-7d8e-4adc-ce5e-6e4d22d6b8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of examples in trainset : 22967\n",
            "Total no. of examples in validset : 5742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainset.class_to_idx)"
      ],
      "metadata": {
        "id": "SrM66qzviRg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82649bb6-210d-4ac3-b535-69bd459b9aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can view some images and their labels\n",
        "import matplotlib.pyplot as plt\n",
        "idx = np.random.randint(low=0, high=len(trainset)-1)\n",
        "print('chosen index: ', idx)\n",
        "image, label = trainset[idx] # the image has format of h, w, c -> so it have to be reshaped\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(label)"
      ],
      "metadata": {
        "id": "4KPvBCwgiWqI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "474c84a7-20e1-40b1-f559-76e0f314324d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chosen index:  17619\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '5')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's read the dataset using pytorch's dataloader\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "validloader = DataLoader(validset, batch_size=batch_size)\n",
        "print('total batches in trainloader: ', len(trainloader), 'and validloader: ', len(validloader))"
      ],
      "metadata": {
        "id": "5BU0SeIctamL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41348ce4-486e-4076-a9cf-813003b29dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total batches in trainloader:  718 and validloader:  180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the shapes of each batch\n",
        "for images, labels in trainloader:\n",
        "  break\n",
        "print(f'shape of a batch of images: {images.shape}')\n",
        "print(f'shape of a batch of labels: {labels.shape}')"
      ],
      "metadata": {
        "id": "HPjkKbNXsyhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95c306b-34dc-4c50-afa3-723ce77910b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of a batch of images: torch.Size([32, 3, 48, 48])\n",
            "shape of a batch of labels: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a model\n",
        "import timm\n",
        "from torch import nn as nn\n",
        "from torchvision import models\n",
        "class FacialClassification(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(FacialClassification, self).__init__()\n",
        "\n",
        "    self.resnet50 = timm.create_model('resnet50', pretrained=False, num_classes=7)\n",
        "  \n",
        "  def forward(self, images, labels=None):\n",
        "    scores = self.resnet50(images)\n",
        "    if labels!=None:\n",
        "      loss = nn.CrossEntropyLoss()(scores, labels)\n",
        "      return scores, loss\n",
        "    return scores"
      ],
      "metadata": {
        "id": "hZStuyJEtO4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = FacialClassification()\n",
        "model = models.resnet50(pretrained=False)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "mSG4M3c2wAiw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7f2413-e241-42cf-d3b3-91e98515ee66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of the model\n",
        "from tqdm import tqdm\n",
        "def multiclass_accuracy(y_pred,y_true):\n",
        "    top_p,top_class = y_pred.topk(1,dim = 1)\n",
        "    equals = top_class == y_true.view(*top_class.shape)\n",
        "    return torch.mean(equals.type(torch.FloatTensor))"
      ],
      "metadata": {
        "id": "mpHoVI9VwEuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building the train and eval functions (training the model on the training and on the validation set)\n",
        "def train_model(model, dataloader, optimizer, current_epoch):\n",
        "\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  tk = tqdm(dataloader, desc='EPOCH' + '[TRAIN]' + str(current_epoch + 1) + \"/\" + str(epochs))\n",
        "\n",
        "  for t, data in enumerate(tk):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # logits, loss = model(images, labels)\n",
        "    logits = model(images)\n",
        "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_acc += multiclass_accuracy(logits, labels)\n",
        "    tk.set_postfix({'loss': float(total_loss/(t+1)),'acc': float(total_acc/(t+1))})\n",
        "  return total_loss/len(dataloader), total_acc / len(dataloader)\n",
        "\n",
        "def eval_model(model, dataloader, current_epoch):\n",
        "\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  tk = tqdm(dataloader, desc='EPOCH' + '[VALID]' + str(current_epoch + 1) + \"/\" + str(epochs))\n",
        "\n",
        "  for t, data in enumerate(tk):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # logits, loss = model(images, labels)\n",
        "    logits = model(images)\n",
        "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_acc += multiclass_accuracy(logits, labels)\n",
        "    tk.set_postfix({'loss': float(total_loss/(t+1)),'acc': float(total_acc/(t+1))})\n",
        "  return total_loss/len(dataloader), total_acc / len(dataloader)"
      ],
      "metadata": {
        "id": "5gmlWz2fwy9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's train the model\n",
        "from torch import optim\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.5, weight_decay=1e-2, nesterov=True)"
      ],
      "metadata": {
        "id": "rzj7fwZoxLN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = np.Inf\n",
        "for i in range(epochs):\n",
        "  train_loss, train_acc = train_model(model, trainloader, optimizer, i)\n",
        "  valid_loss, valid_acc = eval_model(model, validloader, i)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    torch.save(model.state_dict(), 'best-weights.pt')\n",
        "    print('SAVED BEST WEIGHTS')\n",
        "    best_valid_loss = valid_loss\n",
        "print()\n",
        "print()\n",
        "# print('accuracy on the training set: ', float(train_acc))\n",
        "print('accuracy on the validation set: ', float(valid_acc))"
      ],
      "metadata": {
        "id": "AGamEDaZyPxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f6e026-dca8-4a1c-f08b-2bc9ac69885e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EPOCH[TRAIN]1/3: 100%|██████████| 718/718 [01:47<00:00,  6.70it/s, loss=1.92, acc=0.221]\n",
            "EPOCH[VALID]1/3: 100%|██████████| 180/180 [00:08<00:00, 20.72it/s, loss=1.86, acc=0.212]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAVED BEST WEIGHTS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EPOCH[TRAIN]2/3: 100%|██████████| 718/718 [01:51<00:00,  6.45it/s, loss=1.84, acc=0.235]\n",
            "EPOCH[VALID]2/3: 100%|██████████| 180/180 [00:08<00:00, 20.84it/s, loss=1.82, acc=0.235]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAVED BEST WEIGHTS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EPOCH[TRAIN]3/3: 100%|██████████| 718/718 [01:45<00:00,  6.78it/s, loss=1.83, acc=0.239]\n",
            "EPOCH[VALID]3/3: 100%|██████████| 180/180 [00:08<00:00, 21.14it/s, loss=1.81, acc=0.256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAVED BEST WEIGHTS\n",
            "\n",
            "\n",
            "accuracy on the validation set:  0.25577878952026367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Stb131gs7UoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}